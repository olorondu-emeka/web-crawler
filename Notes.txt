Implementation plan

1. get all links on the rootURL's page
2. filter links for those having the rootURL as a prefix
3. iterate through the filtered links and for each link, create an object like so:
    { url: whatever, links: string[] }
4. create a separate worker thread for performing steps 3 above
5.  return this object to the parent thread and push the result to an array
6. in the parent thread, check that all worker threads have compleded processing before returning the final result
7. refactor solution into a library
8. write unit tests


Edge cases & Todos
1. processed crawling of child links in chunks & series (concurrency)
2. add retry times for failed requests
3. handled duplicate links using a linkMap
4. add chunkSize for batch processing
5. Todo: rate limiter for max number of sites to be visited
6. Todo: write unit tests
7. how to keep track of visited sites